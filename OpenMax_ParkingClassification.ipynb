{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nearby-cedar",
      "metadata": {
        "id": "nearby-cedar"
      },
      "source": [
        "# OpenMax를 적용해 정위치 주차여부 판단하자!\n",
        "\n",
        "주차된 차의 블랙박스로 찍힌 이미지를 이용해서 차가 쏘카존의 정위치에 주차되었는지 아닌지를 판별해보자.\n",
        "\n",
        "단순히 주자창마다의 정위치로 분류를 해버리면 새로운 이미지가 들어왔을 때 제대로 판별이 되지않느다.  \n",
        "그래서 현재의 이미지 판별은 Open Set Recognition문제에 해당한다.  \n",
        "Open Set Recognition 문제를 해결하기 위한 방법 중 초기에 등장한 OpenMax라는 기법을 사용해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secret-password",
      "metadata": {
        "id": "secret-password"
      },
      "source": [
        "## 0. 모듈 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fifty-yesterday",
      "metadata": {
        "id": "fifty-yesterday",
        "outputId": "aa06a06c-6eb1-47de-950a-7d3d50c40f94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from scipy import stats\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "identified-offer",
      "metadata": {
        "id": "identified-offer"
      },
      "source": [
        "## 1. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unusual-history",
      "metadata": {
        "id": "unusual-history"
      },
      "outputs": [],
      "source": [
        "PROJECT_PATH = os.getenv('HOME') + '/aiffel/socar_open_set'\n",
        "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
        "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
        "REJECT_PATH = os.path.join(DATA_PATH, 'reject')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "latest-latino",
      "metadata": {
        "id": "latest-latino"
      },
      "source": [
        "준비된 데이터에는 4곳의 쏘카존이 존재하므로 각 쏘카존에서 2장씩 불러와 출력해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecological-chicken",
      "metadata": {
        "id": "ecological-chicken"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
        "    label_name = os.path.basename(dirpath)\n",
        "    for i, filename in enumerate(filenames):\n",
        "        if i >= 2:\n",
        "            break\n",
        "        images.append(Image.open(os.path.join(dirpath, filename)))\n",
        "        labels.append(os.path.basename(dirpath))\n",
        "\n",
        "def show_samples(images, labels):\n",
        "    fig, _ = plt.subplots(4, 2, figsize=(14, 22))\n",
        "    for ax, img, label in zip(fig.axes, images, labels):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(label)\n",
        "    plt.show()\n",
        "    \n",
        "show_samples(images, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![xu101606815470_590562362](https://user-images.githubusercontent.com/15683086/147848965-db928429-bbd0-4d6d-ba1e-3f4c11eba4eb.jpg)\n",
        "\n",
        " 대체용 이미지 <출처: https://www.bobaedream.co.kr/>"
      ],
      "metadata": {
        "id": "OMsdAIKHQrs0"
      },
      "id": "OMsdAIKHQrs0"
    },
    {
      "cell_type": "markdown",
      "id": "indirect-vampire",
      "metadata": {
        "id": "indirect-vampire"
      },
      "source": [
        "기존에는 리사이즈 후에 중간부분을 잘라서 랜덤하게 좌/우반전, 상/하반전을 주는 전처리작업을 하였다.  \n",
        "여기에 의문이 있는 부분에 대하여 Ablation study를 해보았다.\n",
        "\n",
        "이미지가 1920 * 1080으로 매우 크고, 이미지의 위에는 블랙박스의 UI로 글자가 있고,  \n",
        "블랙박스 특성상 밑에 부분에도 차의 보넷으로 판별에 방해가 되는 부분이 있다.  \n",
        "여기에 굳이 기존에 방법에서 좌/우 이미지를 자르지 않고 상/하만 짤라서 학습시켜보았지만,  \n",
        "기존의 좌/우 상/하를 모두 짤랐을 때보다 오히려 성능이 떨어졌다.  \n",
        "예상으로는 이미지의 양옆이 카메라의 특성으로 생긴 굴곡으로 오히려 문제가 되는것 같다.  \n",
        "\n",
        "두번째로 이미지들이 모두 상/하가 정해져 있기 때문에 상/하 반전을 시키지 않고 학습시켰을 경우,  \n",
        "확실히 train데이터에 대해서는 성능이 올라갔지만, 오히려 test데이터에 대해서 성능이 떨어졌다.  \n",
        "이전에 봤던 https://arxiv.org/abs/1803.07728 논문의 원리처럼 기존의 이미지를 방향이 바뀐 이미지를  \n",
        "알아맞추도록 학습되면서 이미지 자체의 특성을 더 잘 추출하게 되는것이 아닌가 싶다.  \n",
        "\n",
        "결국 이래저래 실험해봤지만, 기존의 방법대로 전처리를 하는것이 성능이 가장좋았다.  \n",
        "위의 샘플로 출력한 이미지들에 대해서 전처리 과정을 거쳐서 출력해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "capital-complement",
      "metadata": {
        "id": "capital-complement"
      },
      "outputs": [],
      "source": [
        "nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
        "sample_transformer = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((320, 320), interpolation=nearest_mode),\n",
        "    torchvision.transforms.CenterCrop((224,224)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomVerticalFlip(),\n",
        "    torchvision.transforms.ColorJitter(),\n",
        "])\n",
        "\n",
        "trans_images = [sample_transformer(img) for img in images]\n",
        "show_samples(trans_images, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![2](https://user-images.githubusercontent.com/15683086/147849001-3c9216bb-facc-4c99-84f6-f9e92a64e486.jpg)\n",
        "\n",
        "\n",
        "대체용 이미지"
      ],
      "metadata": {
        "id": "FqzCakMmRzhI"
      },
      "id": "FqzCakMmRzhI"
    },
    {
      "cell_type": "markdown",
      "id": "absent-shuttle",
      "metadata": {
        "id": "absent-shuttle"
      },
      "source": [
        "판별에 방해가 되던 블랙박스 글자가 사라지고, 224 * 224의 크기에 반전도 잘되었다.\n",
        "\n",
        "다음으로 정위치에 주차되지않은 경우들의 이미지들, Open Set Recognition에서 기존의  \n",
        "분류에 속하지 않는 Unknown 데이터에 해당하는 이미지들이다.  \n",
        "몇장을 샘플로 출력해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "creative-algebra",
      "metadata": {
        "id": "creative-algebra"
      },
      "outputs": [],
      "source": [
        "fig, _ = plt.subplots(2, 3, figsize=(20,8))\n",
        "for dirpath, dirnames, filenames in os.walk(REJECT_PATH):\n",
        "    for ax, filename in zip(fig.axes, filenames):\n",
        "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
        "        ax.set_title(f'size: ({image.width}, {image.height}, {image.getbands()})')\n",
        "        ax.imshow(image)\n",
        "        \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ee-IcyRXQSV1"
      },
      "id": "ee-IcyRXQSV1"
    },
    {
      "cell_type": "markdown",
      "id": "daily-times",
      "metadata": {
        "id": "daily-times"
      },
      "source": [
        "우선 준비된 train/test 이미지들로 데이터셋을 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heavy-smoke",
      "metadata": {
        "id": "heavy-smoke"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(path, batch_size, istrain):\n",
        "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
        "    normalize = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    train_transformer = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((320,320), interpolation=nearest_mode),\n",
        "        torchvision.transforms.CenterCrop((224,224)),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.RandomVerticalFlip(),\n",
        "        torchvision.transforms.ColorJitter(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    test_transformer = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((320,320), interpolation=nearest_mode),\n",
        "        torchvision.transforms.CenterCrop((224,224)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    \n",
        "    if istrain:\n",
        "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
        "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "        \n",
        "    else:\n",
        "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
        "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
        "\n",
        "    return dataloader, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrong-beach",
      "metadata": {
        "id": "wrong-beach",
        "outputId": "7f32f2a6-013e-4244-e82e-bcf9d10774f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target_class_num:  4\n",
            "train:  {'atower_b5': 0, 'balsan_b5': 1, 'balsan_b6': 2, 'dcube_b6': 3}\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
        "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
        "\n",
        "print('target_class_num: ', target_class_num)\n",
        "print('train: ', _train_data.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatal-argument",
      "metadata": {
        "id": "fatal-argument",
        "outputId": "1f41454b-fbe1-4308-bf60-e2344a93e829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balsan_b6 : 1250\n",
            "atower_b5 : 1250\n",
            "balsan_b5 : 1250\n",
            "dcube_b6 : 1250\n"
          ]
        }
      ],
      "source": [
        "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
        "    if len(filenames) > 0:\n",
        "        print(f'{os.path.basename(dirpath)} : {len(filenames)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "antique-honey",
      "metadata": {
        "id": "antique-honey"
      },
      "source": [
        "각 라벨마다 1250개 씩 총 5000개의 학습데이터가 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hungarian-reader",
      "metadata": {
        "scrolled": true,
        "id": "hungarian-reader",
        "outputId": "d87276e0-415c-4d28-82fd-049ad774db58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test:  {'atower_b5': 0, 'balsan_b5': 1, 'balsan_b6': 2, 'dcube_b6': 3}\n"
          ]
        }
      ],
      "source": [
        "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
        "print('test: ', _test_data.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "considered-intention",
      "metadata": {
        "id": "considered-intention",
        "outputId": "42cdc46b-33eb-4c2e-baee-3e6a06245353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balsan_b6 : 250\n",
            "atower_b5 : 250\n",
            "balsan_b5 : 250\n",
            "dcube_b6 : 250\n"
          ]
        }
      ],
      "source": [
        "for dirpath, dirnames, filenames in os.walk(TEST_PATH):\n",
        "    if len(filenames) > 0:\n",
        "        print(f'{os.path.basename(dirpath)} : {len(filenames)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amino-security",
      "metadata": {
        "id": "amino-security"
      },
      "source": [
        "250개씩 총 1000개의 테스트데이터가 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "personal-bottom",
      "metadata": {
        "id": "personal-bottom"
      },
      "source": [
        "## 2. 모델 훈련\n",
        "\n",
        "OpenMax 첫번째 단계로 기존 이미지들에 대한 Activation Vector를 얻어야한다.  \n",
        "Activation Vector는 일반적인 딥러닝 분류를 통해 학습된 모델의 마지막 출력에서  \n",
        "softmax를 통과하기 전의 라벨 개수 만큼의 차원을 가지는 벡터를 의미한다.\n",
        "\n",
        "따라서 기존 이미지의 특성을 잘 나타내는 Activation Vector을 얻기 위해서는  \n",
        "일반적인 closed set의 이미지 분류로 방법으로 모델을 학습시켜야한다.  \n",
        "\n",
        "이미지넷으로 pretrained된 resnet50모델을 사용해서 tranfer learning을 사용한다.  \n",
        "출력은 학습데이터의 라벨종류와 같은 4개이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seeing-harassment",
      "metadata": {
        "scrolled": true,
        "id": "seeing-harassment",
        "outputId": "75b4592a-8edf-4fa8-8a3b-0bc3d1755412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = torchvision.models.resnet50(pretrained=True)\n",
        "net.fc = torch.nn.Linear(\n",
        "    net.fc.in_features,\n",
        "    target_class_num\n",
        ")\n",
        "\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incomplete-negative",
      "metadata": {
        "id": "incomplete-negative"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(trues, preds):\n",
        "    accuracy = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average='macro')\n",
        "    precision = precision_score(trues, preds, average='macro')\n",
        "    recall = recall_score(trues, preds, average='macro')\n",
        "    return accuracy, f1, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "driving-credits",
      "metadata": {
        "id": "driving-credits"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
        "    \n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        net.parameters(),\n",
        "        lr = learning_rate, \n",
        "        weight_decay = weight_decay_level\n",
        "    )\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    train_losses = list()\n",
        "    train_preds = list()\n",
        "    train_trues = list()\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataloader):\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = net(img)\n",
        "\n",
        "        _, pred = torch.max(out, 1)\n",
        "        loss = criterion(out, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
        "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
        "\n",
        "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
        "\n",
        "    print('\\n''====== Training Metrics ======')\n",
        "    print('Loss: ', mean(train_losses))\n",
        "    print('Acc: ', acc)\n",
        "    print('F1: ', f1)\n",
        "    print('Precision: ', prec)\n",
        "    print('Recall: ', rec)\n",
        "    print(confusion_matrix(train_trues, train_preds))\n",
        "\n",
        "    return net, acc, f1, prec, rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "toxic-bishop",
      "metadata": {
        "id": "toxic-bishop"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, net, device):\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    net.eval()\n",
        "    test_losses = list()\n",
        "    test_trues = list()\n",
        "    test_preds = list()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, (img, label) in enumerate(dataloader):\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            out = net(img)\n",
        "\n",
        "            _, pred = torch.max(out, 1)\n",
        "            loss = criterion(out, label)\n",
        "\n",
        "            test_losses.append(loss.item())\n",
        "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
        "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
        "\n",
        "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
        "\n",
        "    print('====== Test Metrics ======')\n",
        "    print('Test Loss: ', mean(test_losses))\n",
        "    print('Test Acc: ', acc)\n",
        "    print('Test F1: ', f1)\n",
        "    print('Test Precision: ', prec)\n",
        "    print('Test Recall: ', rec)\n",
        "    print(confusion_matrix(test_trues, test_preds))\n",
        "\n",
        "    return net, acc, f1, prec, rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stuffed-contact",
      "metadata": {
        "id": "stuffed-contact"
      },
      "outputs": [],
      "source": [
        "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
        "    best_test_acc = 0\n",
        "    \n",
        "    model_save_path = None\n",
        "    model_save_base = 'weights4'\n",
        "    if not os.path.exists(model_save_base):\n",
        "        os.makedirs(model_save_base)\n",
        "    \n",
        "    print('>> Start Training Model!')\n",
        "    for epoch in range(n_epochs):\n",
        "        print('> epoch: ', epoch)\n",
        "\n",
        "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
        "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "\n",
        "            best_test_acc = test_acc\n",
        "            test_acc_str = '%.5f' % test_acc\n",
        "\n",
        "            print('[Notification] Best Model Updated!')\n",
        "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
        "            torch.save(net.state_dict(), model_save_path)\n",
        "                \n",
        "    return model_save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "varied-horse",
      "metadata": {
        "scrolled": false,
        "id": "varied-horse",
        "outputId": "ae01d2e8-0a78-4000-d87a-770bb4e87c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Start Training Model!\n",
            "> epoch:  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.9447447253933435\n",
            "Acc:  0.5842\n",
            "F1:  0.5808838417128303\n",
            "Precision:  0.5970113380982228\n",
            "Recall:  0.5841999999999999\n",
            "[[918 157  28 147]\n",
            " [ 49 665 267 269]\n",
            " [ 47 498 419 286]\n",
            " [ 47 181 103 919]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  2.367547545911125\n",
            "Test Acc:  0.412\n",
            "Test F1:  0.32982128136807104\n",
            "Test Precision:  0.5324981954913623\n",
            "Test Recall:  0.41200000000000003\n",
            "[[ 31  18  22 179]\n",
            " [  0 115   8 127]\n",
            " [  0 171  16  63]\n",
            " [  0   0   0 250]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  1\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.47590120438533495\n",
            "Acc:  0.7678\n",
            "F1:  0.7674182311659401\n",
            "Precision:  0.7672194664268897\n",
            "Recall:  0.7678\n",
            "[[1181   19    7   43]\n",
            " [  20  699  475   56]\n",
            " [   8  440  795    7]\n",
            " [  27   51    8 1164]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Test Metrics ======\n",
            "Test Loss:  2.617554460987449\n",
            "Test Acc:  0.298\n",
            "Test F1:  0.19145340939734584\n",
            "Test Precision:  0.3645055583208893\n",
            "Test Recall:  0.29800000000000004\n",
            "[[  0   4 246   0]\n",
            " [  0  29 221   0]\n",
            " [  0   0 250   0]\n",
            " [  0 213  18  19]]\n",
            "> epoch:  2\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.31870494405680067\n",
            "Acc:  0.8768\n",
            "F1:  0.8756299428982298\n",
            "Precision:  0.8784908196476662\n",
            "Recall:  0.8767999999999999\n",
            "[[1205   15   12   18]\n",
            " [  16  883  297   54]\n",
            " [   3  130 1112    5]\n",
            " [  12   47    7 1184]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  1.648326583189957\n",
            "Test Acc:  0.602\n",
            "Test F1:  0.586790565693404\n",
            "Test Precision:  0.7742125747059958\n",
            "Test Recall:  0.6020000000000001\n",
            "[[250   0   0   0]\n",
            " [144  68  26  12]\n",
            " [160   2  88   0]\n",
            " [ 54   0   0 196]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  3\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.17789588274457788\n",
            "Acc:  0.9386\n",
            "F1:  0.9382532209759239\n",
            "Precision:  0.9389103886409715\n",
            "Recall:  0.9386000000000001\n",
            "[[1221    4    6   19]\n",
            " [  10 1077  119   44]\n",
            " [   3   49 1198    0]\n",
            " [  16   33    4 1197]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.4505307664686797\n",
            "Test Acc:  0.854\n",
            "Test F1:  0.8430948008341357\n",
            "Test Precision:  0.8863512291673987\n",
            "Test Recall:  0.854\n",
            "[[250   0   0   0]\n",
            " [  5 122 109  14]\n",
            " [  6   0 244   0]\n",
            " [  4   3   5 238]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  4\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.1369354707672249\n",
            "Acc:  0.9576\n",
            "F1:  0.9575059135046524\n",
            "Precision:  0.9577242586631176\n",
            "Recall:  0.9576\n",
            "[[1233    0    7   10]\n",
            " [   2 1141   76   31]\n",
            " [   0   37 1211    2]\n",
            " [  16   28    3 1203]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.5417565988915394\n",
            "Test Acc:  0.848\n",
            "Test F1:  0.8410414419729293\n",
            "Test Precision:  0.8996230900499337\n",
            "Test Recall:  0.848\n",
            "[[247   0   3   0]\n",
            " [  0 119 130   1]\n",
            " [  0   0 250   0]\n",
            " [  3   2  13 232]]\n",
            "> epoch:  5\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.09228828619528986\n",
            "Acc:  0.9706\n",
            "F1:  0.9705673430843715\n",
            "Precision:  0.9708720785365739\n",
            "Recall:  0.9706\n",
            "[[1231    0    8   11]\n",
            " [   1 1171   63   15]\n",
            " [   4   21 1221    4]\n",
            " [  10    9    1 1230]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  1.4428804323414932\n",
            "Test Acc:  0.706\n",
            "Test F1:  0.6512591054237276\n",
            "Test Precision:  0.7960365909668976\n",
            "Test Recall:  0.706\n",
            "[[230   0   0  20]\n",
            " [  0 205   1  44]\n",
            " [  1 184  27  38]\n",
            " [  0   6   0 244]]\n",
            "> epoch:  6\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.10423084210510118\n",
            "Acc:  0.98\n",
            "F1:  0.9800054657108663\n",
            "Precision:  0.9800510746002864\n",
            "Recall:  0.98\n",
            "[[1235    0    7    8]\n",
            " [   0 1206   35    9]\n",
            " [   4   19 1226    1]\n",
            " [   4   13    0 1233]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  6.004798401789958\n",
            "Test Acc:  0.303\n",
            "Test F1:  0.20188235104101593\n",
            "Test Precision:  0.6394638184871325\n",
            "Test Recall:  0.30300000000000005\n",
            "[[ 11 177   0  62]\n",
            " [  0 249   0   1]\n",
            " [  0 231  19   0]\n",
            " [  0 226   0  24]]\n",
            "> epoch:  7\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.06621625506071539\n",
            "Acc:  0.975\n",
            "F1:  0.9749607810956553\n",
            "Precision:  0.9750365997818292\n",
            "Recall:  0.975\n",
            "[[1238    1    6    5]\n",
            " [   7 1184   44   15]\n",
            " [   2   21 1227    0]\n",
            " [   7   17    0 1226]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.4310532258295555\n",
            "Test Acc:  0.892\n",
            "Test F1:  0.8908309639038273\n",
            "Test Precision:  0.9057548870986645\n",
            "Test Recall:  0.892\n",
            "[[250   0   0   0]\n",
            " [ 21 196  21  12]\n",
            " [ 51   0 196   3]\n",
            " [  0   0   0 250]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  8\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.06722579370370677\n",
            "Acc:  0.9782\n",
            "F1:  0.9782072079933108\n",
            "Precision:  0.9782401310581562\n",
            "Recall:  0.9782\n",
            "[[1235    0    1   14]\n",
            " [   1 1203   38    8]\n",
            " [   0   27 1222    1]\n",
            " [   5   12    2 1231]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.34602505398567945\n",
            "Test Acc:  0.872\n",
            "Test F1:  0.8707064046842876\n",
            "Test Precision:  0.8849944909613059\n",
            "Test Recall:  0.8720000000000001\n",
            "[[247   0   3   0]\n",
            " [  2 184  64   0]\n",
            " [  0   0 250   0]\n",
            " [  4  49   6 191]]\n",
            "> epoch:  9\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.08652050039860644\n",
            "Acc:  0.9678\n",
            "F1:  0.9677800099286804\n",
            "Precision:  0.9678956185176506\n",
            "Recall:  0.9678\n",
            "[[1232    1    6   11]\n",
            " [   4 1172   63   11]\n",
            " [   4   38 1205    3]\n",
            " [   8    8    4 1230]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.13182884569962447\n",
            "Test Acc:  0.951\n",
            "Test F1:  0.9510741613619177\n",
            "Test Precision:  0.9534556242713762\n",
            "Test Recall:  0.951\n",
            "[[250   0   0   0]\n",
            " [  2 236  12   0]\n",
            " [ 27   0 223   0]\n",
            " [  2   6   0 242]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  10\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.04264473329781543\n",
            "Acc:  0.9858\n",
            "F1:  0.9857994405844157\n",
            "Precision:  0.9858243976032972\n",
            "Recall:  0.9858\n",
            "[[1238    1    4    7]\n",
            " [   3 1219   23    5]\n",
            " [   2   12 1235    1]\n",
            " [   4    7    2 1237]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.058614350745191376\n",
            "Test Acc:  0.987\n",
            "Test F1:  0.9870067976930228\n",
            "Test Precision:  0.9871836852722243\n",
            "Test Recall:  0.987\n",
            "[[247   0   2   1]\n",
            " [  0 247   3   0]\n",
            " [  0   0 250   0]\n",
            " [  0   7   0 243]]\n",
            "[Notification] Best Model Updated!\n",
            "> epoch:  11\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.03867606717586235\n",
            "Acc:  0.9888\n",
            "F1:  0.9887981782860356\n",
            "Precision:  0.9888042333144171\n",
            "Recall:  0.9888\n",
            "[[1244    0    1    5]\n",
            " [   0 1225   18    7]\n",
            " [   1   13 1236    0]\n",
            " [   4    5    2 1239]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.0773212887059622\n",
            "Test Acc:  0.973\n",
            "Test F1:  0.9730355080952692\n",
            "Test Precision:  0.974426618525331\n",
            "Test Recall:  0.973\n",
            "[[250   0   0   0]\n",
            " [  0 230  20   0]\n",
            " [  2   0 248   0]\n",
            " [  0   3   2 245]]\n",
            "> epoch:  12\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.07307407287202917\n",
            "Acc:  0.9848\n",
            "F1:  0.9847950915235404\n",
            "Precision:  0.9848000768436551\n",
            "Recall:  0.9848\n",
            "[[1241    0    3    6]\n",
            " [   1 1217   27    5]\n",
            " [   2   20 1226    2]\n",
            " [   6    4    0 1240]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.6805040127088169\n",
            "Test Acc:  0.827\n",
            "Test F1:  0.8115738750623362\n",
            "Test Precision:  0.8818707896411115\n",
            "Test Recall:  0.827\n",
            "[[249   0   0   1]\n",
            " [  0  92 158   0]\n",
            " [  0   0 250   0]\n",
            " [  0   7   7 236]]\n",
            "> epoch:  13\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.03976368652420897\n",
            "Acc:  0.9872\n",
            "F1:  0.9872070913756028\n",
            "Precision:  0.9872190914223951\n",
            "Recall:  0.9872000000000001\n",
            "[[1239    0    2    9]\n",
            " [   0 1229   17    4]\n",
            " [   1   18 1231    0]\n",
            " [   4    8    1 1237]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.31969153133326694\n",
            "Test Acc:  0.899\n",
            "Test F1:  0.8981535907452968\n",
            "Test Precision:  0.9156428031070052\n",
            "Test Recall:  0.899\n",
            "[[250   0   0   0]\n",
            " [  1 175  74   0]\n",
            " [  0   0 250   0]\n",
            " [  2  14  10 224]]\n",
            "> epoch:  14\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.04721327310964418\n",
            "Acc:  0.9862\n",
            "F1:  0.9862064728573102\n",
            "Precision:  0.9862244046118847\n",
            "Recall:  0.9862\n",
            "[[1239    0    3    8]\n",
            " [   0 1223   22    5]\n",
            " [   0   17 1233    0]\n",
            " [   5    7    2 1236]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.22461954587199773\n",
            "Test Acc:  0.91\n",
            "Test F1:  0.9106666913765595\n",
            "Test Precision:  0.9259077779861611\n",
            "Test Recall:  0.91\n",
            "[[197   0  43  10]\n",
            " [  0 220  30   0]\n",
            " [  0   0 250   0]\n",
            " [  0   7   0 243]]\n",
            "> epoch:  15\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.041878883662055943\n",
            "Acc:  0.9868\n",
            "F1:  0.9868050824912207\n",
            "Precision:  0.9868215631525408\n",
            "Recall:  0.9868\n",
            "[[1245    1    2    2]\n",
            " [   1 1219   26    4]\n",
            " [   0   21 1229    0]\n",
            " [   3    3    3 1241]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.0651809423462907\n",
            "Test Acc:  0.975\n",
            "Test F1:  0.9750306286627154\n",
            "Test Precision:  0.9768149848872355\n",
            "Test Recall:  0.975\n",
            "[[245   0   2   3]\n",
            " [  0 230  20   0]\n",
            " [  0   0 250   0]\n",
            " [  0   0   0 250]]\n",
            "> epoch:  16\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.035181919593198954\n",
            "Acc:  0.9906\n",
            "F1:  0.9905998578198298\n",
            "Precision:  0.9906088715130357\n",
            "Recall:  0.9906\n",
            "[[1245    0    3    2]\n",
            " [   1 1235   12    2]\n",
            " [   0   20 1227    3]\n",
            " [   1    2    1 1246]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.22786436172538138\n",
            "Test Acc:  0.931\n",
            "Test F1:  0.9296604536798796\n",
            "Test Precision:  0.9459247648902821\n",
            "Test Recall:  0.931\n",
            "[[250   0   0   0]\n",
            " [  0 181  69   0]\n",
            " [  0   0 250   0]\n",
            " [  0   0   0 250]]\n",
            "> epoch:  17\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.028216121955538947\n",
            "Acc:  0.9924\n",
            "F1:  0.9924005815650649\n",
            "Precision:  0.9924117427654539\n",
            "Recall:  0.9924\n",
            "[[1246    0    1    3]\n",
            " [   0 1232   17    1]\n",
            " [   0    8 1241    1]\n",
            " [   4    3    0 1243]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  2.6979669553140067\n",
            "Test Acc:  0.608\n",
            "Test F1:  0.5591820649664129\n",
            "Test Precision:  0.7568115472528849\n",
            "Test Recall:  0.608\n",
            "[[136  23  88   3]\n",
            " [  0 247   3   0]\n",
            " [  0  48 202   0]\n",
            " [  0 227   0  23]]\n",
            "> epoch:  18\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.05382690426769749\n",
            "Acc:  0.9852\n",
            "F1:  0.9852000000000001\n",
            "Precision:  0.9852000000000001\n",
            "Recall:  0.9852000000000001\n",
            "[[1238    1    4    7]\n",
            " [   2 1226   19    3]\n",
            " [   7   19 1223    1]\n",
            " [   3    4    4 1239]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.07015213928533161\n",
            "Test Acc:  0.976\n",
            "Test F1:  0.9758328366659376\n",
            "Test Precision:  0.9771281263712155\n",
            "Test Recall:  0.976\n",
            "[[248   0   2   0]\n",
            " [  0 229   7  14]\n",
            " [  0   0 249   1]\n",
            " [  0   0   0 250]]\n",
            "> epoch:  19\n",
            "\n",
            "====== Training Metrics ======\n",
            "Loss:  0.042509753096412535\n",
            "Acc:  0.9904\n",
            "F1:  0.9904036637413227\n",
            "Precision:  0.9904108196027105\n",
            "Recall:  0.9904000000000001\n",
            "[[1242    0    4    4]\n",
            " [   0 1236   12    2]\n",
            " [   2   13 1235    0]\n",
            " [   5    4    2 1239]]\n",
            "====== Test Metrics ======\n",
            "Test Loss:  0.22933309419060183\n",
            "Test Acc:  0.914\n",
            "Test F1:  0.91541106743202\n",
            "Test Precision:  0.92595838069976\n",
            "Test Recall:  0.9139999999999999\n",
            "[[219   0  29   2]\n",
            " [  0 215  35   0]\n",
            " [  0   0 250   0]\n",
            " [  0  19   1 230]]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.005\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conscious-collaboration",
      "metadata": {
        "id": "conscious-collaboration",
        "outputId": "aaed294c-1cfa-4672-c1ec-9dee9ae03197"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#saved_weight_path = 'weights4/classifier_acc_0.99300.pth'\n",
        "saved_weight_path = os.path.join(MODEL_PATH, 'classifier_acc_0.96008.pth')\n",
        "net.load_state_dict(torch.load(saved_weight_path, map_location=device))\n",
        "net.eval()\n",
        "\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "virgin-words",
      "metadata": {
        "id": "virgin-words"
      },
      "source": [
        "## 3. Activation Vector 추출\n",
        "\n",
        "미리 학습된 모델을 이용해서 Activation Vector를 추출해보자.  \n",
        "현재의 학습 데이터는 여러 augmentation을 거친 데이터이기 때문에 새롭게 만들어준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "several-overall",
      "metadata": {
        "id": "several-overall"
      },
      "outputs": [],
      "source": [
        "train_loader, _train_data = create_dataloader(TRAIN_PATH, 1, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "temporal-automation",
      "metadata": {
        "id": "temporal-automation"
      },
      "source": [
        "augmentation을 거치지 않은 데이터를 모델의 출력인 activation vector로 만들어서 리스트에 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nutritional-safety",
      "metadata": {
        "id": "nutritional-safety"
      },
      "outputs": [],
      "source": [
        "train_preds = list()\n",
        "train_actvecs = list()\n",
        "train_outputs_softmax = list()\n",
        "train_labels = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx, (img, label) in enumerate(train_loader):\n",
        "      img = img.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      out = net(img)\n",
        "      out_actvec = out.cpu().detach().numpy()[0]\n",
        "      out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
        "      out_pred = int(torch.argmax(out).cpu().detach().numpy())\n",
        "      out_label = int(label.cpu().detach().numpy())\n",
        "\n",
        "      train_actvecs.append(out_actvec) # component 1: softmax 전의 Activation Vector\n",
        "      train_preds.append(out_pred) # componenet 2: 각 데이터에 대한 예측값\n",
        "      train_outputs_softmax.append(out_softmax) # component 3: 각 데이터에 대한 softmax 확률\n",
        "      train_labels.append(out_label) # component 4: 각 데이터에 대한 Label (정답)\n",
        "\n",
        "train_actvecs = np.asarray(train_actvecs)\n",
        "train_preds = np.asarray(train_preds)\n",
        "train_outputs_softmax = np.asarray(train_outputs_softmax)\n",
        "train_labels = np.asarray(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unauthorized-metallic",
      "metadata": {
        "id": "unauthorized-metallic"
      },
      "source": [
        "OpenMax는 정답을 맞힌 ativation vector를 사용한다.  \n",
        "만들아진 activation vector 리스트의 정보를 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparable-silly",
      "metadata": {
        "id": "comparable-silly",
        "outputId": "14c2c351-ba74-4d08-a7a8-fb81cee3aa11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activation vector:  (4790, 4)\n",
            "Labels:  (4790,)\n"
          ]
        }
      ],
      "source": [
        "train_correct_actvecs = train_actvecs[train_labels==train_preds]\n",
        "train_correct_labels = train_labels[train_labels==train_preds]\n",
        "print('Activation vector: ', train_correct_actvecs.shape)\n",
        "print('Labels: ', train_correct_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mathematical-writer",
      "metadata": {
        "id": "mathematical-writer"
      },
      "source": [
        "최종적으로 4개의 차원을 가진 4790의 activation vector가 만들어졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cultural-obligation",
      "metadata": {
        "id": "cultural-obligation"
      },
      "source": [
        "## 4. 베이불 분포의 모수 계산\n",
        "\n",
        "다음으로 각 클래스 별 Activation Vector의 평균 계산한다.  \n",
        "그리고 각 Activation Vector별 평균으로부터 가장 먼 100개의 Vector를 이용해  \n",
        "베이불 분포의 모수를 추출한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "attended-speaker",
      "metadata": {
        "id": "attended-speaker",
        "outputId": "f6c3320a-bfcd-4242-b9c8-5224a9c68524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class_idx:  0\n",
            "(1250, 4)\n",
            "class_idx:  1\n",
            "(1170, 4)\n",
            "class_idx:  2\n",
            "(1179, 4)\n",
            "class_idx:  3\n",
            "(1191, 4)\n"
          ]
        }
      ],
      "source": [
        "class_means = list()\n",
        "dist_to_means = list()\n",
        "mr_models = {}\n",
        "\n",
        "for class_idx in np.unique(train_labels):\n",
        "    \n",
        "    print('class_idx: ', class_idx)\n",
        "    class_act_vec = train_correct_actvecs[train_correct_labels==class_idx]\n",
        "    print(class_act_vec.shape)\n",
        "    \n",
        "    class_mean = class_act_vec.mean(axis=0)\n",
        "    class_means.append(class_mean)\n",
        "    \n",
        "    dist_to_mean = np.square(class_act_vec - class_mean).sum(axis=1) # 각 activation vector의 거리를 계산\n",
        "    dist_to_mean_sorted = np.sort(dist_to_mean).astype(np.float64) # 거리를 기준으로 오름차순 정렬\n",
        "    dist_to_means.append(dist_to_mean_sorted)\n",
        "\n",
        "    shape, loc, scale = stats.weibull_max.fit(dist_to_mean[-100:]) # 거리가 가장 먼 100개를 사용하여 모수 추출\n",
        "    \n",
        "    mr_models[str(class_idx)] = {\n",
        "        'shape':shape,\n",
        "        'loc':loc,\n",
        "        'scale':scale\n",
        "    }\n",
        "    \n",
        "class_means = np.asarray(class_means)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "killing-gambling",
      "metadata": {
        "id": "killing-gambling"
      },
      "source": [
        "## 5. OpenMax 적용\n",
        "\n",
        "베이불 분포가 높다는 것은 평범하지 않다는 말이고, 이것는 주차 위치가 오위치에 있다는 뜻이다.  \n",
        "베이불 분포로부터 이미지의 확률을 계산하고 그걸 통해 기존 클래스에 포함 되지 않는  \n",
        "reject 클래스로 분류할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developed-climate",
      "metadata": {
        "id": "developed-climate"
      },
      "outputs": [],
      "source": [
        "def compute_openmax(actvec, class_means, mr_models):\n",
        "    dist_to_mean = np.square(actvec - class_means).sum(axis=1)\n",
        "\n",
        "    scores = list()\n",
        "    for class_idx in range(len(class_means)):\n",
        "        params = mr_models[str(class_idx)]\n",
        "        score = stats.weibull_max.cdf(\n",
        "            dist_to_mean[class_idx],\n",
        "            params['shape'],\n",
        "            params['loc'],\n",
        "            params['scale']\n",
        "        )\n",
        "        scores.append(score)\n",
        "    scores = np.asarray(scores)\n",
        "    \n",
        "    weight_on_actvec = 1 - scores # 각 class별 가중치\n",
        "    rev_actvec = np.concatenate([\n",
        "        weight_on_actvec * actvec, # known class에 대한 가중치 곱\n",
        "        [((1-weight_on_actvec) * actvec).sum()] # unknown class에 새로운 계산식\n",
        "    ])\n",
        "    \n",
        "    openmax_prob = np.exp(rev_actvec) / np.exp(rev_actvec).sum()\n",
        "    return openmax_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "handy-westminster",
      "metadata": {
        "id": "handy-westminster"
      },
      "source": [
        "위의 함수를 통해 reject에 포함될 확률을 구했지만, 계산한 최대 확률이  \n",
        "모두 다 낮은 경우라면 강제로 reject클래스로 분류해주는 과정이 필요하다.\n",
        "\n",
        "다음ㄴ 지정된 threshold 값 보다 작은 경우는 reject클래스로 분류해주는 함수이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "several-picture",
      "metadata": {
        "id": "several-picture"
      },
      "outputs": [],
      "source": [
        "def inference(actvec, threshold, target_class_num, class_means, mr_models):\n",
        "    openmax_prob = compute_openmax(actvec, class_means, mr_models)\n",
        "    openmax_softmax = np.exp(openmax_prob)/sum(np.exp(openmax_prob))\n",
        "\n",
        "    pred = np.argmax(openmax_softmax)\n",
        "    if np.max(openmax_softmax) < threshold:\n",
        "        pred = target_class_num\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conditional-speed",
      "metadata": {
        "id": "conditional-speed"
      },
      "outputs": [],
      "source": [
        "def inference_dataloader(net, data_loader, threshold, target_class_num, class_means, mr_models, is_reject=False):\n",
        "    result_preds = list()\n",
        "    result_labels = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for idx, (img, label) in enumerate(data_loader):\n",
        "          img = img.to(device)\n",
        "          label = label.to(device)\n",
        "\n",
        "          out = net(img)\n",
        "          out_actvec = out.cpu().detach().numpy()[0]\n",
        "          out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
        "          out_label = int(label.cpu().detach().numpy())\n",
        "\n",
        "          pred = inference(out_actvec, threshold, target_class_num, class_means, mr_models)\n",
        "      \n",
        "          result_preds.append(pred)\n",
        "          if is_reject:\n",
        "              result_labels.append(target_class_num)\n",
        "          else:\n",
        "              result_labels.append(out_label)\n",
        "\n",
        "    return result_preds, result_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fatal-auction",
      "metadata": {
        "id": "fatal-auction"
      },
      "source": [
        "## 6. Threshold에 따른 추이를 살펴보자\n",
        "\n",
        "이제 Threshold값을 변경하면서 test와 reject데이터에 대한 성능을 알아보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secure-legislation",
      "metadata": {
        "id": "secure-legislation"
      },
      "outputs": [],
      "source": [
        "test_loader, _test_data = create_dataloader(TEST_PATH, 1, False)\n",
        "reject_loader, _reject_data = create_dataloader(REJECT_PATH, 1, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "divided-terrain",
      "metadata": {
        "id": "divided-terrain"
      },
      "source": [
        "정해진 범위의 threshold 서치를 통해서 정위치 데이터인 test 데이터와   \n",
        "오위치 데이터인 reject 데이터의 정확도를 비교해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fossil-visit",
      "metadata": {
        "id": "fossil-visit"
      },
      "outputs": [],
      "source": [
        "def threshold_search(search_list):\n",
        "    test_predictions = []\n",
        "    reject_predictions = []\n",
        "\n",
        "    for threshold in search_list:\n",
        "        test_preds, test_labels = inference_dataloader(\n",
        "            net, test_loader, threshold, target_class_num, class_means, mr_models)\n",
        "        reject_preds, reject_labels = inference_dataloader(\n",
        "            net, reject_loader, threshold, target_class_num, class_means, mr_models, is_reject=True)\n",
        "        test_predictions.append(accuracy_score(test_labels, test_preds))\n",
        "        reject_predictions.append(accuracy_score(reject_labels, reject_preds))\n",
        "\n",
        "    return test_predictions, reject_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uniform-timeline",
      "metadata": {
        "id": "uniform-timeline"
      },
      "outputs": [],
      "source": [
        "search_list = [x * 0.01 for x in range(20, 51, 5)]\n",
        "\n",
        "test_predictions, reject_predictions = threshold_search(search_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heated-series",
      "metadata": {
        "id": "heated-series",
        "outputId": "88821b57-b319-45b3-beee-7df16a3b7ff0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(search_list, test_predictions)\n",
        "plt.plot(search_list, reject_predictions)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Threshold')\n",
        "plt.legend(['test', 'reject'], loc='lower left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "technological-courtesy",
      "metadata": {
        "id": "technological-courtesy"
      },
      "source": [
        "당연하게 threshold 값이 커지면서 정위치에서 평균에서 거리는가 있는 데이터들이  \n",
        "오위치로 분류되면서 정확도가 떨어지고, 반대로 reject는 커지고 있다.\n",
        "\n",
        "여기서 정위치에 대한 정확도와 오위치에 대한 정확도가 모두 높은 값을 나타내는  \n",
        "threshold는 0.35정도로 보인다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "searching-advertising",
      "metadata": {
        "id": "searching-advertising",
        "outputId": "f2835843-376c-4806-89ef-a3dbf30469ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.851\n",
            "Reject Accuracy:  0.92\n"
          ]
        }
      ],
      "source": [
        "test_preds, test_labels = inference_dataloader(net, test_loader, 0.35, target_class_num, class_means, mr_models)\n",
        "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.35, target_class_num, class_means, mr_models, is_reject=True)\n",
        "\n",
        "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
        "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "soviet-valentine",
      "metadata": {
        "id": "soviet-valentine"
      },
      "source": [
        "## 7. 회고\n",
        "\n",
        "이번 프로젝트를 하면서 처음으로 Open Set Recognition이라는 것을 알았다.  \n",
        "생각해보면 현실은 항상 정해져 있는 답에 속하는 데이터만 들어온다는 것보다  \n",
        "기존에 없던 데이터가 추가로 들어오는 상황이 훨씬 많을 것이다.  \n",
        "\n",
        "특히 이번 프로젝트를 하면서 보통 모델의 softmax를 하기전의 마지막 출력이  \n",
        "단순하게 그 값들에 대한 스코어 이기도 하지만 Activation Vector로 활용될 수  \n",
        "있다는 것에서 여러가지 인사이트를 얻은 것 같다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "OpenMax_ParkingClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}